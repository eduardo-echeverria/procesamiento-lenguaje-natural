{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Procesamiento de Lenguaje Natural\n",
    "### Desafio 1\n",
    "\n",
    "#### Nombre: Eduardo Echeverria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Enunciado:\n",
    "\n",
    "**1**. Vectorizar documentos. Tomar 5 documentos al azar y medir similaridad con el resto de los documentos.\n",
    "Estudiar los 5 documentos más similares de cada uno analizar si tiene sentido\n",
    "la similaridad según el contenido del texto y la etiqueta de clasificación.\n",
    "\n",
    "**2**. Entrenar modelos de clasificación Naïve Bayes para maximizar el desempeño de clasificación\n",
    "(f1-score macro) en el conjunto de datos de test. Considerar cambiar parámteros\n",
    "de instanciación del vectorizador y los modelos y probar modelos de Naïve Bayes Multinomial\n",
    "y ComplementNB.\n",
    "\n",
    "**3**. Transponer la matriz documento-término. De esa manera se obtiene una matriz\n",
    "término-documento que puede ser interpretada como una colección de vectorización de palabras.\n",
    "Estudiar ahora similaridad entre palabras tomando 5 palabras y estudiando sus 5 más similares. **La elección de palabras no debe ser al azar para evitar la aparición de términos poco interpretables, elegirlas \"manualmente\"**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos las librerias necesarias\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.naive_bayes import MultinomialNB, ComplementNB\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1**. Vectorizar documentos. Tomar 5 documentos al azar y medir similaridad con el resto de los documentos.\n",
    "Estudiar los 5 documentos más similares de cada uno analizar si tiene sentido\n",
    "la similaridad según el contenido del texto y la etiqueta de clasificación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Carga de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos los datasets de entrrenamiento y prueba\n",
    "newsgroup_train = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))\n",
    "newsgroup_test = fetch_20newsgroups(subset='test', remove=('headers', 'footers', 'quotes'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorizacion: TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciamos el vectorizador TF-IDF\n",
    "tf_idf_vect = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realizamos el ajuste y transformación de los datos.\n",
    "X_train = tf_idf_vect.fit_transform(newsgroup_train.data)\n",
    "y_train = newsgroup_train.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse._csr.csr_matrix'>\n",
      "Tamaño del dataset: (11314, 101631)\n",
      "Cantidad de documentos: 11314\n",
      "Tamaño del vocabulario: 101631\n"
     ]
    }
   ],
   "source": [
    "# Mostramos el tamaño del dataset, cantidad de documentos y tamaño del vocabulario.\n",
    "print(type(X_train))\n",
    "print(f'Tamaño del dataset: {X_train.shape}')\n",
    "print(f'Cantidad de documentos: {X_train.shape[0]}')\n",
    "print(f'Tamaño del vocabulario: {X_train.shape[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Similaridad:\n",
    "\n",
    "Utilizando el vectorizador TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7629, 410, 2010, 1885, 3308]\n"
     ]
    }
   ],
   "source": [
    "# Generamos aleatoriamente 5 números entre el rango correspondiente a la cantidad de documentos.\n",
    "document_index_list = []\n",
    "for num in range(5):\n",
    "    rand_int = random.randint(1, X_train.shape[0])\n",
    "    document_index_list.append(rand_int)\n",
    "\n",
    "print(document_index_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "~~~~~~~~~~ INICIO DEL DOCUMENTO 7629 ~~~~~~~~~~\n",
      "\n",
      "This announcement is somewhat disconcerting; it doesn't do anything\n",
      "evil in and of itself, but bodes badly for the future of open\n",
      "algorithms and standards in information security.  I won't start\n",
      "panicking until/unless DES or RSA or stuff like that is prohibited, but\n",
      "I'm a little anxious.  (No doubt it won't be long before someone posts\n",
      "explaining how this just a small part of some far-ranging and\n",
      "long-lived NSA-PKP-IRS-FBI-CIA-HandgunControlInc-Clinton conspiracy to\n",
      "subvert freedom, democracy, and mathematics.)  My feeling is that the\n",
      "administration probably isn't that worried about things like DES and\n",
      "RSA and PGP and RIPEM, since they'll never be used by a group much\n",
      "wider than us computer geeks.\n",
      "\n",
      "The fact that this just came out now suggests one of two things:\n",
      "\n",
      "1.  The NSA has been working on this for a long time, and it only just\n",
      "    now happened to be ``ready'' to release to the world at this time.\n",
      "\n",
      "2.  The NSA has been working on this for a long time, but wasn't able\n",
      "    to get the Bush administration to go along with this plan.  (I\n",
      "    find it unlikely that this would have been because of a sympathy\n",
      "    for the unescrowed use of cryptography; more likely the\n",
      "    administration felt that even escrowed, secret-algorithm and, for\n",
      "    all we know, trivially breakable cryptography should not be made\n",
      "    widely available.)\n",
      "\n",
      "Thus said clipper@csrc.ncsl.nist.gov (Clipper Chip Announcement):\n",
      "\n",
      "The majority of the discussion involving this \"Clipper Chip\" seems to\n",
      "pertain to the encryption of telephone conversations.  Does anyone\n",
      "know if that means this chip is designed to work primarily with analog\n",
      "signals?  The language sort of suggests this, but it's hard to say.\n",
      "\n",
      "The main thing I just don't get is whether this chip implements\n",
      "symmetric or asymmetric cryptographic techniques.  Anybody know?\n",
      "\n",
      "I'm guessing symmetric, but they don't get very clear about it.  If it\n",
      "is symmetric, how is it useful for anything other than link-level\n",
      "encryption with an identical chip at each end?  How can you negotiate\n",
      "a per-session key using symmetric cryptography without using a trusted\n",
      "third party who knows your key?  (Or does it even use a per-session\n",
      "key?)\n",
      "\n",
      "If it's asymmetric, what about PKP's patents, which they claim cover\n",
      "all methods of doing asymmetric cryptography?  Are they getting\n",
      "royalties, or is hiding infringement the real reason for keeping the\n",
      "algorithm secret? :-)\n",
      "~~~~~~~~~~ FIN DEL DOCUMENTO 7629 ~~~~~~~~~~\n",
      "\n",
      "Analisis del Documento: 7629\n",
      "\n",
      "Indices de los 5 documentos mas similares: [10261 10575  6243  2825  5856]\n",
      "La clase del documento original es: sci.crypt\n",
      "Las clases de los 5 documentos mas similares son:\n",
      "sci.crypt\n",
      "sci.crypt\n",
      "sci.crypt\n",
      "sci.crypt\n",
      "sci.crypt\n",
      "\n",
      "~~~~~~~~~~ INICIO DEL DOCUMENTO 410 ~~~~~~~~~~\n",
      "\n",
      "\n",
      " Any projectile traveling at or near typical bullet speeds is potentially\n",
      "lethal.  Even blanks [which have no projectile] can cause death if the\n",
      "muzzle is in close proximity to the victim.  I have heard of rubber or\n",
      "plastic bullets being used effectively during riot situations [where the\n",
      "intent is crowd control, rather than close range self defense]; i've also\n",
      "seen reports of deaths caused by them [the British in Northern Ireland].\n",
      " Use of a firearm for self defense is appropriate and lawful only in the\n",
      "gravest of situations; at that point, i consider deadly [lethal] force to \n",
      "be a proper reaction [and so does the law].  \n",
      " Furthermore, use of less effective [but still potentially lethal] force\n",
      "has its own set of problems.  It may well take more applications of the\n",
      "less effective force to stop the incident; this places all parties at some\n",
      "risk; the victim because the attack has not stopped, and the assailent \n",
      "since the aggregate damage done by the multiple applications may well be\n",
      "more deadly.\n",
      "\n",
      "~~~~~~~~~~ FIN DEL DOCUMENTO 410 ~~~~~~~~~~\n",
      "\n",
      "Analisis del Documento: 410\n",
      "\n",
      "Indices de los 5 documentos mas similares: [ 6704  6537  2190  5945 10836]\n",
      "La clase del documento original es: talk.politics.guns\n",
      "Las clases de los 5 documentos mas similares son:\n",
      "sci.space\n",
      "talk.politics.guns\n",
      "talk.politics.guns\n",
      "talk.politics.guns\n",
      "alt.atheism\n",
      "\n",
      "~~~~~~~~~~ INICIO DEL DOCUMENTO 2010 ~~~~~~~~~~\n",
      "\n",
      "\n",
      "this borders on blasphemy.\n",
      "~~~~~~~~~~ FIN DEL DOCUMENTO 2010 ~~~~~~~~~~\n",
      "\n",
      "Analisis del Documento: 2010\n",
      "\n",
      "Indices de los 5 documentos mas similares: [9748 7433 8919 1866 5740]\n",
      "La clase del documento original es: rec.sport.baseball\n",
      "Las clases de los 5 documentos mas similares son:\n",
      "talk.politics.mideast\n",
      "talk.politics.mideast\n",
      "soc.religion.christian\n",
      "talk.politics.mideast\n",
      "talk.politics.mideast\n",
      "\n",
      "~~~~~~~~~~ INICIO DEL DOCUMENTO 1885 ~~~~~~~~~~\n",
      "\n",
      "\n",
      "[...]\n",
      "\n",
      "   In the September 1992 issue of THE TUFTS UNIVERSITY DIET AND NUTRITION\n",
      "   LETTER, there is a three page article about artificial sweeteners.  What\n",
      "   follows are those excerpts which deal specifically with Nutrasweet.\n",
      "\n",
      "   [Reproduced without permission]\n",
      "\n",
      "\t   The controversy [over aspartame] began six years ago in England,\n",
      "\twhere a group of researchers found that aspartame, marketed under\n",
      "\tthe tradename Nutrasweet, appears to stimulate appetite and,\n",
      "\tpresumably, the eating of more calories in the long run than if\n",
      "\ta person simply consumed sugar.  When researchers asked a group\n",
      "\tof 95 people to drink plain water, aspartame-sweetened water, and\n",
      "\tsugared water, they said that overall they felt hungriest after\n",
      "\tdrinking the artificially sweetened beverage.\n",
      "\t   The study received widespread media attention and stirred a\n",
      "\tgood deal of concern among the artificial-sweetener-using public.\n",
      "\tHowever, its results were questionable at best, since the researchers\n",
      "\tdid not go on to measure whether the increase in appetite did\n",
      "\tactually translate into an increase in eating.  The two do not\n",
      "\tnecessarily go hand in hand.\n",
      "\t   In the years that followed, more than a dozen studies examining\n",
      "\tthe effect of aspartame on appetite -- and eating -- were conducted.\n",
      "\tAnd after reviewing every one of them, the director of the\n",
      "\tLaboratory of the Study of Human Ingestive Behavior at Johns Hopkins\n",
      "\tUniversity, Barbara Rolls, Ph.D., concluded that consuming aspartame-\n",
      "\tsweetened foods and drinks is not associated with any increase in\n",
      "\tthe amount of food eaten afterward.\n",
      "\n",
      "\t   One artificial sweetener that is not typically accused of causing\n",
      "\tcancer is aspartame.  But it most certainly has been blamed for a\n",
      "\thost of other ills.  Since its introduction in 1981, the government\n",
      "\thas received thousands of complaints accusing it of causing\n",
      "\teverything from headaches to nausea to mood swings to anxiety.\n",
      "\tStill, years of careful scientific study conducted both before and\n",
      "\tafter the sweetener's entering the market have failed to confirm\n",
      "\tthat it can bring about adverse health effects.  That's why the\n",
      "\tCenters for Disease Control (the government agency charged with\n",
      "\tmonitoring public health), the American Medical Association's\n",
      "\tCouncil on Scientific Affairs, and the Food and Drug Administration\n",
      "\thave given aspartame, one of the most studied food additives, a\n",
      "\tclean bill of health.\n",
      "\t   Granted, the FDA has set forth an \"acceptable daily intake\" of\n",
      "\t50 milligrams of aspartame per kilogram of body weight.  To exceed\n",
      "\tthe limit, however, a 120-pound (55 kg.) woman would have to take\n",
      "\tin 2,750 milligrams of aspartame -- the amount in 15 cans of\n",
      "\taspartame-sweetened soda pop, 14 cups of gelatin, 22 cups of yogurt,\n",
      "\tor 55 six-ounce servings of aspartame-containing hot cocoa,...\n",
      "\tA 175-pound (80 kg.) man would have to consume some 4,000 milligrams\n",
      "\tof the sweetener -- the amount in 22 cans of soda pop or 32 cups\n",
      "\tof yogurt -- to go over the limit.  [chart with aspartame content\n",
      "\tof selected foods omitted]\n",
      "\t   Only one small group of people must be certain to stay away\n",
      "\tfrom aspartame: those born with a rare metabolic disorder called\n",
      "\tphenylketonuria, or PKU.  The estimated one person in every 12,000\n",
      "\tto 15,000 who has it is unable to properly metabolize an essential\n",
      "\tamino acid in aspartame called phenylalanine.  Once a child\n",
      "\tconsumes it, it builds up in the body and can ultimately cause\n",
      "\tsuch severe problems as mental retardation.  To help people with\n",
      "\tPKU avoid the substance, labels on cans of soda pop and other\n",
      "\taspartame-sweetened foods must carry the warning \"Phenylketonurics:\n",
      "\tContains Phenylalanine.\"\n",
      "\n",
      "~~~~~~~~~~ FIN DEL DOCUMENTO 1885 ~~~~~~~~~~\n",
      "\n",
      "Analisis del Documento: 1885\n",
      "\n",
      "Indices de los 5 documentos mas similares: [  913  6894 10836  8615  4988]\n",
      "La clase del documento original es: sci.med\n",
      "Las clases de los 5 documentos mas similares son:\n",
      "alt.atheism\n",
      "talk.politics.guns\n",
      "alt.atheism\n",
      "talk.politics.mideast\n",
      "sci.med\n",
      "\n",
      "~~~~~~~~~~ INICIO DEL DOCUMENTO 3308 ~~~~~~~~~~\n",
      "\n",
      "\n",
      "\n",
      "Does that imply that people who take marriage vows but aren't sincere\n",
      "are not married?\n",
      "~~~~~~~~~~ FIN DEL DOCUMENTO 3308 ~~~~~~~~~~\n",
      "\n",
      "Analisis del Documento: 3308\n",
      "\n",
      "Indices de los 5 documentos mas similares: [7847 9894 5025 8825 9198]\n",
      "La clase del documento original es: soc.religion.christian\n",
      "Las clases de los 5 documentos mas similares son:\n",
      "soc.religion.christian\n",
      "soc.religion.christian\n",
      "soc.religion.christian\n",
      "soc.religion.christian\n",
      "soc.religion.christian\n"
     ]
    }
   ],
   "source": [
    "# Calculamos la similaridad Coseno, la clase del documento original y las 5 clases mas similares.\n",
    "for index in document_index_list:\n",
    "    print(f\"\\n~~~~~~~~~~ INICIO DEL DOCUMENTO {index} ~~~~~~~~~~\\n\")\n",
    "    print(newsgroup_train.data[index])\n",
    "    print(f\"~~~~~~~~~~ FIN DEL DOCUMENTO {index} ~~~~~~~~~~\\n\")\n",
    "    \n",
    "    # Calculamos la similaridad Coseno\n",
    "    cos_sim = cosine_similarity(X_train[index], X_train)[0]\n",
    "    most_similar = np.argsort(cos_sim)[::-1][1:6]\n",
    "    original_class = newsgroup_train.target_names[y_train[index]]\n",
    "\n",
    "    # Dezplegamos los resultados\n",
    "    print(f\"Analisis del Documento: {index}\\n\")\n",
    "    print(f\"Indices de los 5 documentos mas similares: {most_similar}\")\n",
    "    print(f\"La clase del documento original es: {original_class}\")\n",
    "    print(f\"Las clases de los 5 documentos mas similares son:\")\n",
    "\n",
    "    for i in most_similar:\n",
    "        print(newsgroup_train.target_names[y_train[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Análisis\n",
    "\n",
    "Luego de realizada la prueba se puede observar lo siguiente.\n",
    "Para el documento 7629, el modelo lo clasifica como de la clase \"SCI.CRYPT\" es decir como un articulo relacionado con conocimiento científico criptografico, lo cual es correcto dado que menciona conceptos de encriptacion simétrica y asimétrica, así como diferentes algoritmos de encriptación. Tambien se observa que el modelo identifica que todos los articulos mas similares son de la misma clase \"SCI.CRYPT\", por tanto la clasificación es correcta.\n",
    "\n",
    "Para el documento 410, el artículo brinda una descripción técnica acerca armas de fuego. El modelo clasifica acertadamente el artículo dentro de la clase \"TALK.POLITICS.GUNS\". Para los documentos más similares, la mayoría (3 de 5) corresponden a la misma clase, sin embargo tambien aparecen la clase \"SCI.SPACE\" lo cual puede deberse a algunos términos técnicos. También aparece la clase \"ALT.ATHEISM\", la cual parece ser una clase mucho menos relacionada con el tema del artículo y su clase original. En general el modelo clasificó bien el docuemnto y los documentos más similares.\n",
    "\n",
    "El documento 2010, contiene unicamente 4 palabras, lo cual hace difícil clasificarlo acertadamente. El modelo le asigna la clase original de \"REC.SPORT.BASEBALL\" probablemente por la palabra \"borders\", sin embargo esta clase no resulta del todo acertada. Entre las clases de los documentos más similares, aparece la clase \"SOC.RELIGION.CHRISTIAN\", la cual tiene sentido considerando la palabra \"blasphemy\". En general, para este documento el modelo no realizó una clsificacion acertda, pero nuevamente se debe considerar que el documento contiene muy pocas palabras significativas (unicamente \"borders\" y \"blasphemy\" ya que \"this\" y \"on\" no proporcionan mucho contexto).\n",
    "\n",
    "El documento 1885 trata un tema relacionado con substancias endulzantes, los efectos en la salud y las recomendaciones de la FDA de Estados Unidos. El modelo clasifica el artículo dentro de la clase \"SCI.MED\" lo cual es acertado. Sin embargo se observa que las clases de los documentos más similares son muy variadas, siendo solamente uno de ellos de la misma clase \"SCI.MED\". Las otras clases, no guardan mucha relación con el tema tratado en el documento.\n",
    "\n",
    "Finalmente, el documento 3308, si bien es también corto, proporciona el suficiente contexto para entender que se discute acerca del valor de los votos matrimoniales. El modelo clasifica el documento dentro de la clase \"SOC.RELIGION.CHRISTIAN\" lo cual es acertado. De igual forma, el modelo identifica con la misma clase a todos los documentos mas similares.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2**. Entrenar modelos de clasificación Naïve Bayes para maximizar el desempeño de clasificación\n",
    "(f1-score macro) en el conjunto de datos de test. Considerar cambiar parámteros\n",
    "de instanciación del vectorizador y los modelos y probar modelos de Naïve Bayes Multinomial\n",
    "y ComplementNB."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorizador TF-IDF optimizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilizamos los siguientes parametros para instanciar el vectorizador:\n",
    "#   sublinear_tf: True\n",
    "#   ngram_range: (1, 1)\n",
    "#   max_df: 0.95\n",
    "#   min_df: 1\n",
    "#   stop_words: 'english'\n",
    "\n",
    "tf_idf_vect = TfidfVectorizer(sublinear_tf=True, max_df=0.5, min_df=5, ngram_range=(1, 1), stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realizamos el ajuste y transformación de los datos.\n",
    "X_train_tf = tf_idf_vect.fit_transform(newsgroup_train.data)\n",
    "X_test_tf = tf_idf_vect.transform(newsgroup_test.data)\n",
    "y_train_tf = newsgroup_train.target\n",
    "y_test_tf = newsgroup_test.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modelo Naïve-Bayes Multinomial\n",
    "\n",
    "Aplicado al vectorizador TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El mejor F1-score es: 0.6820089088033873, obtenido con el modelo ComplementNB\n"
     ]
    }
   ],
   "source": [
    "# Definimos los modelos a entrenar. \n",
    "naive_bayes_models = {\n",
    "    'MultinomialNB': MultinomialNB(),\n",
    "    'ComplementNB': ComplementNB(),\n",
    "}\n",
    "\n",
    "# Definimos algunos valores de inicio\n",
    "best_f1 = 0\n",
    "best_model = None\n",
    "\n",
    "for model, model_instance in naive_bayes_models.items():\n",
    "    model_instance.fit(X_train_tf, y_train_tf)\n",
    "    y_pred_tf = model_instance.predict(X_test_tf)\n",
    "    f1 = f1_score(y_test_tf, y_pred_tf, average='macro')\n",
    "\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_model = model\n",
    "\n",
    "print(f\"El mejor F1-score es: {best_f1}, obtenido con el modelo {best_model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Análisis\n",
    "\n",
    "Luego de probar con diferentes valores para los parámetros del vectorizador, encontramos que los mejores resultados se obtienen con los siguientes parámetros:\n",
    "- sublinear_tf: True\n",
    "- ngram_range: (1, 1)\n",
    "- max_df: 0.95\n",
    "- min_df: 1\n",
    "- stop_words: 'english'\n",
    "\n",
    "La obtención de estos valores fue totalmente experimental.\n",
    "\n",
    "Se observa también que el modelo que mejores resultados, en cuanto al valor del F1-Score es el \"Complement Naïve-Bayes\". El mejor valor obtenido es de 0.68\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3**. Transponer la matriz documento-término. De esa manera se obtiene una matriz\n",
    "término-documento que puede ser interpretada como una colección de vectorización de palabras.\n",
    "Estudiar ahora similaridad entre palabras tomando 5 palabras y estudiando sus 5 más similares. **La elección de palabras no debe ser al azar para evitar la aparición de términos poco interpretables, elegirlas \"manualmente\"**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilizamos las mismas configuraciones del vectorizador TF-IDF y Count del primer ejercicio.\n",
    "tf_idf_vect = TfidfVectorizer(sublinear_tf=True, max_df=0.5, min_df=5, ngram_range=(1, 1), stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Volvemos a realizar el ajuste y transformación de los datos.\n",
    "X_tf_idf = tf_idf_vect.fit_transform(newsgroup_train.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension de la matriz original: (11314, 17797)\n",
      "Dimension de la matriz transpuesta: (17797, 11314)\n"
     ]
    }
   ],
   "source": [
    "# Transponemos la matriz Documento-Término para obtener la matriz Término-Documento\n",
    "transposed_matrix = X_tf_idf.transpose()\n",
    "\n",
    "print(f\"Dimension de la matriz original: {X_tf_idf.shape}\")\n",
    "print(F\"Dimension de la matriz transpuesta: {transposed_matrix.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtenemos los feature names, es decir los términos\n",
    "feature_names = tf_idf_vect.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleccionamos 5 términos. En este caso, términos relacionados con autos y motores\n",
    "words_to_analyze = [\"science\", \"software\", \"christianity\", \"engine\", \"citizens\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Palabra analizada: science\n",
      "Palabras mas similares:\n",
      "\n",
      "scientific\n",
      "cognitivists\n",
      "behaviorists\n",
      "scientists\n",
      "empirical\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Palabra analizada: software\n",
      "Palabras mas similares:\n",
      "\n",
      "hardware\n",
      "pc\n",
      "packages\n",
      "maturity\n",
      "use\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Palabra analizada: christianity\n",
      "Palabras mas similares:\n",
      "\n",
      "christian\n",
      "christians\n",
      "religion\n",
      "versed\n",
      "religions\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Palabra analizada: engine\n",
      "Palabras mas similares:\n",
      "\n",
      "exhaust\n",
      "clamp\n",
      "rebuilt\n",
      "oil\n",
      "household\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Palabra analizada: citizens\n",
      "Palabras mas similares:\n",
      "\n",
      "abiding\n",
      "felony\n",
      "arrest\n",
      "government\n",
      "granting\n"
     ]
    }
   ],
   "source": [
    "for word in words_to_analyze:\n",
    "    index = tf_idf_vect.vocabulary_.get(word)\n",
    "    if index:\n",
    "        cosine_sim = cosine_similarity(transposed_matrix[index].reshape(1, -1), transposed_matrix)[0]\n",
    "        most_sim = np.argsort(cosine_sim)[::-1][1:6]\n",
    "        print(\"~\"*50)\n",
    "        print(f\"Palabra analizada: {word}\")\n",
    "        print(\"Palabras mas similares:\\n\")\n",
    "\n",
    "        for i in most_sim:\n",
    "            print(feature_names[i])\n",
    "    else:\n",
    "        print(f\"\\nPalabra: {word} no encontrada\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Análisis\n",
    "\n",
    "Se puede observar lo siguiente:\n",
    "\n",
    "Para la palabra seleccionada \"Science\" o ciencia, las palabras mas similares encontradas por el modelo guardan estrecha relación con las palabras más similares.\n",
    "\n",
    "Para la palabra \"Software\", el modelo identifica palabras muy relacionadas con el mundo de la computación, sin embargo también identifica palabras como \"maturity\" y \"use\" que pueden resultar un poco ambiguas, comparadas con el contexto de la palabra \"Software\".\n",
    "\n",
    "La tercera palabra, \"Christianity\" se encuentra completamente relacionada con las palabras identificadas por el modelo, las cuales son todas relacionadas con el tema religioso cristiano.\n",
    "\n",
    "La palabra \"Engine\" o motor, se encuentra también bastante relacionada con las palabras identificadas por el modelo como las más similares. Todas estas palabras se relacionan con maquinaria, vehículos, aceite, etc. Quizás la mas cuestionable es la palabra \"household\" la cual está mas relacionada con el contexto del hogar, lo cual llega a ser un poco ambiguo en el contexto de la palabra original \"Engine\".\n",
    "\n",
    "Finalmente, para la palabra \"Citizens\" o ciudadanos. El modelo identifica otras palabras que guardan relación con la palabra original, sin embargo esto sucede dependiendo mucho dentro del contexto mas relacionado con asuntos legales o gubernamentales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sandbox_ai_env",
   "language": "python",
   "name": "sandbox_ai_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
